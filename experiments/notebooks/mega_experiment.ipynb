{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mega Experiment: Crossâ€Validation, Model Comparison and Results Visualization\n",
    "\n",
    "In this notebook we perform a cross-validation experiment using several forecaster models available in APDTFlow. We:\n",
    "- Clean and load the time series dataset.\n",
    "- Create sliding-window datasets.\n",
    "- Define a rolling-window cross-validation splitting strategy.\n",
    "- Train multiple models (APDTFlow, TransformerForecaster, TCNForecaster, EnsembleForecaster) on each fold.\n",
    "- Collect evaluation metrics.\n",
    "- Save the results to a CSV summary.\n",
    "- Plot the forecasts (sample) as well as a bar plot to compare average performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: C:\\Users\\yotam\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.chdir(\"C:/Users/yotam/code_projects/APDTFlow\")\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from apdtflow.data import TimeSeriesWindowDataset\n",
    "from apdtflow.models.apdtflow import APDTFlow\n",
    "try:\n",
    "    from apdtflow.models.transformer_forecaster import TransformerForecaster\n",
    "    from apdtflow.models.tcn_forecaster import TCNForecaster\n",
    "    from apdtflow.models.ensemble_forecaster import EnsembleForecaster\n",
    "except ImportError:\n",
    "    print(\"Warning: One or more alternative forecaster modules are not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean CSV saved to: C:/Users/yotam/code_projects/APDTFlow/dataset_examples/daily-minimum-temperatures-in-me_clean.csv\n"
     ]
    }
   ],
   "source": [
    "original_csv = \"C:/Users/yotam/code_projects/APDTFlow/dataset_examples/daily-minimum-temperatures-in-me.csv\"\n",
    "clean_csv = \"C:/Users/yotam/code_projects/APDTFlow/dataset_examples/daily-minimum-temperatures-in-me_clean.csv\"\n",
    "df = pd.read_csv(original_csv)\n",
    "df[\"Daily minimum temperatures\"] = pd.to_numeric(df[\"Daily minimum temperatures\"], errors='coerce')\n",
    "df = df.dropna(subset=[\"Daily minimum temperatures\"])\n",
    "df[\"Daily minimum temperatures\"] = df[\"Daily minimum temperatures\"].astype(np.float32)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "df.to_csv(clean_csv, index=False)\n",
    "print(\"Clean CSV saved to:\", clean_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_params(csv_file, value_col):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[value_col] = df[value_col].astype(np.float32)\n",
    "    values = df[value_col].values\n",
    "    return values.mean(), values.std()\n",
    "\n",
    "mean_val, std_val = compute_norm_params(clean_csv, \"Daily minimum temperatures\")\n",
    "\n",
    "def normalize_tensor(x):\n",
    "    return (x - mean_val) / std_val\n",
    "\n",
    "def denormalize_tensor(x):\n",
    "    return x * std_val + mean_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_forward(model, x_batch, t_span, device):\n",
    "    model_name = model.__class__.__name__\n",
    "    if model_name == 'APDTFlow':\n",
    "        return model(x_batch, t_span)\n",
    "    elif model_name == 'TransformerForecaster':\n",
    "        if x_batch.dim() == 2:\n",
    "            x_batch = x_batch.unsqueeze(-1) \n",
    "        elif x_batch.dim() == 3 and x_batch.size(1) == 1:\n",
    "            x_batch = x_batch.transpose(1, 2) \n",
    "        return model(x_batch)\n",
    "    elif model_name == 'TCNForecaster':\n",
    "        if x_batch.dim() == 2:\n",
    "            x_batch = x_batch.unsqueeze(1) \n",
    "        return model(x_batch)\n",
    "    elif model_name == 'EnsembleForecaster':\n",
    "        return model.predict(x_batch, None, device)\n",
    "    else:\n",
    "        return model(x_batch, t_span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_splits(dataset, train_size, step_size, max_splits=3):\n",
    "    \"\"\"\n",
    "    Yields up to max_splits train and validation indices for a rolling window split.\n",
    "    \"\"\"\n",
    "    n_samples = len(dataset)\n",
    "    count = 0\n",
    "    for start in range(0, n_samples - train_size, step_size):\n",
    "        if count >= max_splits:\n",
    "            break\n",
    "        train_indices = list(range(start, start + train_size))\n",
    "        val_indices = list(range(start + train_size, min(start + train_size + step_size, n_samples)))\n",
    "        yield train_indices, val_indices\n",
    "        count += 1\n",
    "\n",
    "\n",
    "def save_forecast_plot(x_batch, y_batch, preds, T_in, T_out, save_path, title=\"Forecast\"):\n",
    "    sample_idx = 0\n",
    "    if x_batch.dim() == 3:\n",
    "        x_sample = x_batch[sample_idx, 0, :].cpu().numpy()\n",
    "    else:\n",
    "        x_sample = x_batch[sample_idx].cpu().numpy()\n",
    "    y_sample = y_batch[sample_idx].squeeze().cpu().numpy()\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    if preds.dim() == 3:\n",
    "        pred_sample = preds[sample_idx, :, 0].cpu().numpy()\n",
    "    else:\n",
    "        pred_sample = preds[sample_idx].cpu().numpy()\n",
    "    \n",
    "    x_sample_denorm = denormalize_tensor(torch.tensor(x_sample)).numpy()\n",
    "    y_sample_denorm = denormalize_tensor(torch.tensor(y_sample)).numpy()\n",
    "    pred_sample_denorm = denormalize_tensor(torch.tensor(pred_sample)).numpy()\n",
    "    \n",
    "    input_timesteps = np.arange(T_in)\n",
    "    future_timesteps = np.arange(T_in, T_in+T_out)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(input_timesteps, x_sample_denorm, label=\"Input Sequence\", marker=\"o\")\n",
    "    plt.plot(future_timesteps, y_sample_denorm, label=\"True Future\", marker=\"o\", linestyle=\"--\")\n",
    "    plt.plot(future_timesteps, pred_sample_denorm, label=\"Predicted Future\", marker=\"o\", linestyle=\":\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Daily Minimum Temperature\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(\"Saved forecast plot to:\", save_path)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            if model.__class__.__name__ == 'APDTFlow' and x_batch.dim() == 4 and x_batch.size(1)==1:\n",
    "                x_batch = x_batch.squeeze(1)\n",
    "            batch_size = x_batch.size(0)\n",
    "            if model.__class__.__name__ == 'APDTFlow':\n",
    "                T_in_current = x_batch.size(-1)\n",
    "            else:\n",
    "                if x_batch.dim() == 2:\n",
    "                    T_in_current = x_batch.size(1)\n",
    "                elif x_batch.dim() == 3:\n",
    "                    T_in_current = x_batch.size(-1)\n",
    "            t_span = torch.linspace(0,1,steps=T_in_current, device=device)\n",
    "            output = run_model_forward(model, x_batch, t_span, device)\n",
    "            if isinstance(output, tuple):\n",
    "                preds = output[0]\n",
    "            else:\n",
    "                preds = output\n",
    "            mse = (preds - y_batch.transpose(1,2)) ** 2\n",
    "            loss = torch.mean(mse)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def train_on_split(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            if model.__class__.__name__ == 'APDTFlow' and x_batch.dim()==4 and x_batch.size(1)==1:\n",
    "                x_batch = x_batch.squeeze(1)\n",
    "            \n",
    "            batch_size = x_batch.size(0)\n",
    "            if model.__class__.__name__ == 'APDTFlow':\n",
    "                T_in_current = x_batch.size(-1)\n",
    "            else:\n",
    "                if x_batch.dim() == 2:\n",
    "                    T_in_current = x_batch.size(1)\n",
    "                elif x_batch.dim() == 3:\n",
    "                    T_in_current = x_batch.size(-1)\n",
    "            t_span = torch.linspace(0, 1, steps=T_in_current, device=device)\n",
    "            optimizer.zero_grad()\n",
    "            output = run_model_forward(model, x_batch, t_span, device)\n",
    "            \n",
    "            if model.__class__.__name__ == 'APDTFlow':\n",
    "                preds, pred_logvars = output\n",
    "                mse = (preds - y_batch.transpose(1,2)) ** 2\n",
    "                loss = torch.mean(0.5 * (mse / (pred_logvars.exp() + 1e-6)) + 0.5 * pred_logvars)\n",
    "            else:\n",
    "                if isinstance(output, tuple):\n",
    "                    preds = output[0]\n",
    "                else:\n",
    "                    preds = output\n",
    "                mse = (preds - y_batch.transpose(1,2)) ** 2\n",
    "                loss = torch.mean(mse)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "        avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            if model.__class__.__name__ == 'APDTFlow' and x_batch.dim()==4 and x_batch.size(1)==1:\n",
    "                x_batch = x_batch.squeeze(1)\n",
    "            batch_size = x_batch.size(0)\n",
    "            if model.__class__.__name__ == 'APDTFlow':\n",
    "                T_in_current = x_batch.size(-1)\n",
    "            else:\n",
    "                if x_batch.dim() == 2:\n",
    "                    T_in_current = x_batch.size(1)\n",
    "                elif x_batch.dim() == 3:\n",
    "                    T_in_current = x_batch.size(-1)\n",
    "            t_span = torch.linspace(0, 1, steps=T_in_current, device=device)\n",
    "            output = run_model_forward(model, x_batch, t_span, device)\n",
    "            \n",
    "            if model.__class__.__name__ == 'APDTFlow':\n",
    "                preds, pred_logvars = output\n",
    "                mse = (preds - y_batch.transpose(1,2)) ** 2\n",
    "                loss = torch.mean(0.5 * (mse / (pred_logvars.exp() + 1e-6)) + 0.5 * pred_logvars)\n",
    "            else:\n",
    "                if isinstance(output, tuple):\n",
    "                    preds = output[0]\n",
    "                else:\n",
    "                    preds = output\n",
    "                mse = (preds - y_batch.transpose(1,2)) ** 2\n",
    "                loss = torch.mean(mse)\n",
    "            val_loss += loss.item() * batch_size\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizons = [7, 14, 30]\n",
    "T_in = 30          \n",
    "train_size = 400  \n",
    "step_size = 50  \n",
    "num_epochs = 35\n",
    "learning_rate = 0.0007\n",
    "batch_size = 16\n",
    "\n",
    "apdtflow_params = {\n",
    "    \"num_scales\": 3,         # you might experiment with 4 scales too\n",
    "    \"input_channels\": 1,\n",
    "    \"filter_size\": 7,         # increased from 5 to 7\n",
    "    \"hidden_dim\": 32,         # increased from 16 to 32\n",
    "    \"output_dim\": 1,\n",
    "}\n",
    "\n",
    "# For TransformerForecaster, increase the model dimension and layers:\n",
    "transformer_params = {\n",
    "    \"input_dim\": 1,\n",
    "    \"model_dim\": 32,        # increased from 16 to 32\n",
    "    \"num_layers\": 2,        # increased from 1 to 2\n",
    "    \"nhead\": 4,\n",
    "    \"forecast_horizon\": None\n",
    "}\n",
    "\n",
    "# For TCNForecaster, try more channels and a larger kernel:\n",
    "tcn_params = {\n",
    "    \"input_channels\": 1,\n",
    "    \"num_channels\": [32, 32],  # increased from [16,16]\n",
    "    \"kernel_size\": 7,          # increased from 5 to 7\n",
    "    \"forecast_horizon\": None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir = \"C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "results_dir = os.path.join(project_root, \"experiments\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "results_csv_path = os.path.join(results_dir, \"results_experiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Forecast Horizon (T_out): 7\n",
      "==========================\n",
      "\n",
      "Dataset loaded. Total samples: 3611\n",
      "\n",
      "--- CV Split 1 ---\n",
      "\n",
      "Training APDTFlow model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yotam\\code_projects\\APDTFlow\\venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Training Loss: 14.3999\n",
      "Epoch 2/35, Training Loss: 5.2590\n",
      "Epoch 3/35, Training Loss: 4.0981\n",
      "Epoch 4/35, Training Loss: 3.6622\n",
      "Epoch 5/35, Training Loss: 3.4077\n",
      "Epoch 6/35, Training Loss: 3.2272\n",
      "Epoch 7/35, Training Loss: 3.1022\n",
      "Epoch 8/35, Training Loss: 3.0157\n",
      "Epoch 9/35, Training Loss: 2.9490\n",
      "Epoch 10/35, Training Loss: 2.8979\n",
      "Epoch 11/35, Training Loss: 2.8542\n",
      "Epoch 12/35, Training Loss: 2.8185\n",
      "Epoch 13/35, Training Loss: 2.7868\n",
      "Epoch 14/35, Training Loss: 2.7565\n",
      "Epoch 15/35, Training Loss: 2.7300\n",
      "Epoch 16/35, Training Loss: 2.7036\n",
      "Epoch 17/35, Training Loss: 2.6770\n",
      "Epoch 18/35, Training Loss: 2.6504\n",
      "Epoch 19/35, Training Loss: 2.6271\n",
      "Epoch 20/35, Training Loss: 2.5978\n",
      "Epoch 21/35, Training Loss: 2.5684\n",
      "Epoch 22/35, Training Loss: 2.5423\n",
      "Epoch 23/35, Training Loss: 2.5119\n",
      "Epoch 24/35, Training Loss: 2.4837\n",
      "Epoch 25/35, Training Loss: 2.4527\n",
      "Epoch 26/35, Training Loss: 2.4189\n",
      "Epoch 27/35, Training Loss: 2.3878\n",
      "Epoch 28/35, Training Loss: 2.3537\n",
      "Epoch 29/35, Training Loss: 2.3169\n",
      "Epoch 30/35, Training Loss: 2.2887\n",
      "Epoch 31/35, Training Loss: 2.2521\n",
      "Epoch 32/35, Training Loss: 2.2181\n",
      "Epoch 33/35, Training Loss: 2.1868\n",
      "Epoch 34/35, Training Loss: 2.1516\n",
      "Epoch 35/35, Training Loss: 2.1190\n",
      "Validation Loss: 2.0642\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 109.6288\n",
      "Epoch 2/35, Training Loss: 83.9643\n",
      "Epoch 3/35, Training Loss: 73.7559\n",
      "Epoch 4/35, Training Loss: 64.8551\n",
      "Epoch 5/35, Training Loss: 56.5250\n",
      "Epoch 6/35, Training Loss: 48.9710\n",
      "Epoch 7/35, Training Loss: 42.3035\n",
      "Epoch 8/35, Training Loss: 36.6009\n",
      "Epoch 9/35, Training Loss: 31.9135\n",
      "Epoch 10/35, Training Loss: 28.1723\n",
      "Epoch 11/35, Training Loss: 25.3021\n",
      "Epoch 12/35, Training Loss: 23.2085\n",
      "Epoch 13/35, Training Loss: 21.7367\n",
      "Epoch 14/35, Training Loss: 20.8138\n",
      "Epoch 15/35, Training Loss: 20.4511\n",
      "Epoch 16/35, Training Loss: 20.0199\n",
      "Epoch 17/35, Training Loss: 18.8878\n",
      "Epoch 18/35, Training Loss: 16.5217\n",
      "Epoch 19/35, Training Loss: 15.0134\n",
      "Epoch 20/35, Training Loss: 13.6256\n",
      "Epoch 21/35, Training Loss: 12.7833\n",
      "Epoch 22/35, Training Loss: 12.0339\n",
      "Epoch 23/35, Training Loss: 11.5634\n",
      "Epoch 24/35, Training Loss: 11.1964\n",
      "Epoch 25/35, Training Loss: 10.8713\n",
      "Epoch 26/35, Training Loss: 10.6274\n",
      "Epoch 27/35, Training Loss: 10.3982\n",
      "Epoch 28/35, Training Loss: 10.3255\n",
      "Epoch 29/35, Training Loss: 10.0494\n",
      "Epoch 30/35, Training Loss: 9.9647\n",
      "Epoch 31/35, Training Loss: 9.7423\n",
      "Epoch 32/35, Training Loss: 9.7353\n",
      "Epoch 33/35, Training Loss: 9.5206\n",
      "Epoch 34/35, Training Loss: 9.5734\n",
      "Epoch 35/35, Training Loss: 9.3629\n",
      "Validation Loss: 12.3740\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 122.8966\n",
      "Epoch 2/35, Training Loss: 31.8308\n",
      "Epoch 3/35, Training Loss: 21.4734\n",
      "Epoch 4/35, Training Loss: 12.6756\n",
      "Epoch 5/35, Training Loss: 14.1924\n",
      "Epoch 6/35, Training Loss: 12.5990\n",
      "Epoch 7/35, Training Loss: 11.7567\n",
      "Epoch 8/35, Training Loss: 12.0734\n",
      "Epoch 9/35, Training Loss: 11.1524\n",
      "Epoch 10/35, Training Loss: 11.0660\n",
      "Epoch 11/35, Training Loss: 11.2361\n",
      "Epoch 12/35, Training Loss: 10.6168\n",
      "Epoch 13/35, Training Loss: 10.6635\n",
      "Epoch 14/35, Training Loss: 10.7385\n",
      "Epoch 15/35, Training Loss: 10.3156\n",
      "Epoch 16/35, Training Loss: 10.3806\n",
      "Epoch 17/35, Training Loss: 10.3573\n",
      "Epoch 18/35, Training Loss: 10.0710\n",
      "Epoch 19/35, Training Loss: 10.1234\n",
      "Epoch 20/35, Training Loss: 10.0500\n",
      "Epoch 21/35, Training Loss: 9.8796\n",
      "Epoch 22/35, Training Loss: 9.8834\n",
      "Epoch 23/35, Training Loss: 9.8032\n",
      "Epoch 24/35, Training Loss: 9.6647\n",
      "Epoch 25/35, Training Loss: 9.6727\n",
      "Epoch 26/35, Training Loss: 9.5826\n",
      "Epoch 27/35, Training Loss: 9.5017\n",
      "Epoch 28/35, Training Loss: 9.4609\n",
      "Epoch 29/35, Training Loss: 9.3817\n",
      "Epoch 30/35, Training Loss: 9.3030\n",
      "Epoch 31/35, Training Loss: 9.2632\n",
      "Epoch 32/35, Training Loss: 9.1946\n",
      "Epoch 33/35, Training Loss: 9.1369\n",
      "Epoch 34/35, Training Loss: 9.0650\n",
      "Epoch 35/35, Training Loss: 9.0177\n",
      "Validation Loss: 13.3098\n",
      "\n",
      "EnsembleForecaster CV Loss: 11.7325\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_7_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_7_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_7_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_7_CV1.png\n",
      "\n",
      "--- CV Split 2 ---\n",
      "\n",
      "Training APDTFlow model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yotam\\code_projects\\APDTFlow\\venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Training Loss: 22.0252\n",
      "Epoch 2/35, Training Loss: 5.4423\n",
      "Epoch 3/35, Training Loss: 4.3343\n",
      "Epoch 4/35, Training Loss: 3.9347\n",
      "Epoch 5/35, Training Loss: 3.6662\n",
      "Epoch 6/35, Training Loss: 3.4723\n",
      "Epoch 7/35, Training Loss: 3.3153\n",
      "Epoch 8/35, Training Loss: 3.1930\n",
      "Epoch 9/35, Training Loss: 3.0986\n",
      "Epoch 10/35, Training Loss: 3.0255\n",
      "Epoch 11/35, Training Loss: 2.9652\n",
      "Epoch 12/35, Training Loss: 2.9131\n",
      "Epoch 13/35, Training Loss: 2.8717\n",
      "Epoch 14/35, Training Loss: 2.8335\n",
      "Epoch 15/35, Training Loss: 2.8005\n",
      "Epoch 16/35, Training Loss: 2.7683\n",
      "Epoch 17/35, Training Loss: 2.7401\n",
      "Epoch 18/35, Training Loss: 2.7090\n",
      "Epoch 19/35, Training Loss: 2.6790\n",
      "Epoch 20/35, Training Loss: 2.6470\n",
      "Epoch 21/35, Training Loss: 2.6144\n",
      "Epoch 22/35, Training Loss: 2.5831\n",
      "Epoch 23/35, Training Loss: 2.5487\n",
      "Epoch 24/35, Training Loss: 2.5118\n",
      "Epoch 25/35, Training Loss: 2.4753\n",
      "Epoch 26/35, Training Loss: 2.4318\n",
      "Epoch 27/35, Training Loss: 2.3916\n",
      "Epoch 28/35, Training Loss: 2.3508\n",
      "Epoch 29/35, Training Loss: 2.3063\n",
      "Epoch 30/35, Training Loss: 2.2628\n",
      "Epoch 31/35, Training Loss: 2.2230\n",
      "Epoch 32/35, Training Loss: 2.1819\n",
      "Epoch 33/35, Training Loss: 2.1406\n",
      "Epoch 34/35, Training Loss: 2.1010\n",
      "Epoch 35/35, Training Loss: 2.0625\n",
      "Validation Loss: 1.9047\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 94.9566\n",
      "Epoch 2/35, Training Loss: 67.1137\n",
      "Epoch 3/35, Training Loss: 57.7135\n",
      "Epoch 4/35, Training Loss: 49.6454\n",
      "Epoch 5/35, Training Loss: 42.4772\n",
      "Epoch 6/35, Training Loss: 36.2838\n",
      "Epoch 7/35, Training Loss: 31.1305\n",
      "Epoch 8/35, Training Loss: 26.9918\n",
      "Epoch 9/35, Training Loss: 23.8508\n",
      "Epoch 10/35, Training Loss: 21.5264\n",
      "Epoch 11/35, Training Loss: 19.8714\n",
      "Epoch 12/35, Training Loss: 18.8226\n",
      "Epoch 13/35, Training Loss: 18.2212\n",
      "Epoch 14/35, Training Loss: 17.9803\n",
      "Epoch 15/35, Training Loss: 17.3184\n",
      "Epoch 16/35, Training Loss: 15.4958\n",
      "Epoch 17/35, Training Loss: 13.8120\n",
      "Epoch 18/35, Training Loss: 12.8184\n",
      "Epoch 19/35, Training Loss: 12.0691\n",
      "Epoch 20/35, Training Loss: 11.5207\n",
      "Epoch 21/35, Training Loss: 11.1995\n",
      "Epoch 22/35, Training Loss: 10.8796\n",
      "Epoch 23/35, Training Loss: 10.7009\n",
      "Epoch 24/35, Training Loss: 10.5642\n",
      "Epoch 25/35, Training Loss: 10.3264\n",
      "Epoch 26/35, Training Loss: 10.2464\n",
      "Epoch 27/35, Training Loss: 10.1670\n",
      "Epoch 28/35, Training Loss: 10.0374\n",
      "Epoch 29/35, Training Loss: 9.8978\n",
      "Epoch 30/35, Training Loss: 9.8560\n",
      "Epoch 31/35, Training Loss: 9.7558\n",
      "Epoch 32/35, Training Loss: 9.6105\n",
      "Epoch 33/35, Training Loss: 9.5979\n",
      "Epoch 34/35, Training Loss: 9.5218\n",
      "Epoch 35/35, Training Loss: 9.5117\n",
      "Validation Loss: 13.6849\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 98.7838\n",
      "Epoch 2/35, Training Loss: 30.0594\n",
      "Epoch 3/35, Training Loss: 33.7672\n",
      "Epoch 4/35, Training Loss: 17.2195\n",
      "Epoch 5/35, Training Loss: 18.6712\n",
      "Epoch 6/35, Training Loss: 14.7807\n",
      "Epoch 7/35, Training Loss: 13.7774\n",
      "Epoch 8/35, Training Loss: 12.9721\n",
      "Epoch 9/35, Training Loss: 12.4610\n",
      "Epoch 10/35, Training Loss: 12.4056\n",
      "Epoch 11/35, Training Loss: 12.0133\n",
      "Epoch 12/35, Training Loss: 11.8944\n",
      "Epoch 13/35, Training Loss: 11.7469\n",
      "Epoch 14/35, Training Loss: 11.5135\n",
      "Epoch 15/35, Training Loss: 11.4691\n",
      "Epoch 16/35, Training Loss: 11.3044\n",
      "Epoch 17/35, Training Loss: 11.1798\n",
      "Epoch 18/35, Training Loss: 11.1505\n",
      "Epoch 19/35, Training Loss: 10.9865\n",
      "Epoch 20/35, Training Loss: 10.9360\n",
      "Epoch 21/35, Training Loss: 10.8542\n",
      "Epoch 22/35, Training Loss: 10.7218\n",
      "Epoch 23/35, Training Loss: 10.7281\n",
      "Epoch 24/35, Training Loss: 10.5241\n",
      "Epoch 25/35, Training Loss: 10.4539\n",
      "Epoch 26/35, Training Loss: 10.4656\n",
      "Epoch 27/35, Training Loss: 10.2528\n",
      "Epoch 28/35, Training Loss: 10.2149\n",
      "Epoch 29/35, Training Loss: 10.1683\n",
      "Epoch 30/35, Training Loss: 9.9851\n",
      "Epoch 31/35, Training Loss: 9.9687\n",
      "Epoch 32/35, Training Loss: 9.8777\n",
      "Epoch 33/35, Training Loss: 9.7534\n",
      "Epoch 34/35, Training Loss: 9.7164\n",
      "Epoch 35/35, Training Loss: 9.6426\n",
      "Validation Loss: 17.0465\n",
      "\n",
      "EnsembleForecaster CV Loss: 14.6971\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_7_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_7_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_7_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_7_CV2.png\n",
      "\n",
      "--- CV Split 3 ---\n",
      "\n",
      "Training APDTFlow model...\n",
      "Epoch 1/35, Training Loss: 26.5855\n",
      "Epoch 2/35, Training Loss: 5.3907\n",
      "Epoch 3/35, Training Loss: 4.3158\n",
      "Epoch 4/35, Training Loss: 3.8385\n",
      "Epoch 5/35, Training Loss: 3.5613\n",
      "Epoch 6/35, Training Loss: 3.3621\n",
      "Epoch 7/35, Training Loss: 3.2153\n",
      "Epoch 8/35, Training Loss: 3.1012\n",
      "Epoch 9/35, Training Loss: 3.0147\n",
      "Epoch 10/35, Training Loss: 2.9462\n",
      "Epoch 11/35, Training Loss: 2.8891\n",
      "Epoch 12/35, Training Loss: 2.8437\n",
      "Epoch 13/35, Training Loss: 2.8035\n",
      "Epoch 14/35, Training Loss: 2.7712\n",
      "Epoch 15/35, Training Loss: 2.7401\n",
      "Epoch 16/35, Training Loss: 2.7123\n",
      "Epoch 17/35, Training Loss: 2.6879\n",
      "Epoch 18/35, Training Loss: 2.6662\n",
      "Epoch 19/35, Training Loss: 2.6440\n",
      "Epoch 20/35, Training Loss: 2.6213\n",
      "Epoch 21/35, Training Loss: 2.5981\n",
      "Epoch 22/35, Training Loss: 2.5750\n",
      "Epoch 23/35, Training Loss: 2.5531\n",
      "Epoch 24/35, Training Loss: 2.5313\n",
      "Epoch 25/35, Training Loss: 2.5076\n",
      "Epoch 26/35, Training Loss: 2.4817\n",
      "Epoch 27/35, Training Loss: 2.4580\n",
      "Epoch 28/35, Training Loss: 2.4322\n",
      "Epoch 29/35, Training Loss: 2.4065\n",
      "Epoch 30/35, Training Loss: 2.3796\n",
      "Epoch 31/35, Training Loss: 2.3552\n",
      "Epoch 32/35, Training Loss: 2.3256\n",
      "Epoch 33/35, Training Loss: 2.2993\n",
      "Epoch 34/35, Training Loss: 2.2742\n",
      "Epoch 35/35, Training Loss: 2.2436\n",
      "Validation Loss: 1.9890\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 94.3139\n",
      "Epoch 2/35, Training Loss: 75.6127\n",
      "Epoch 3/35, Training Loss: 66.1753\n",
      "Epoch 4/35, Training Loss: 57.6627\n",
      "Epoch 5/35, Training Loss: 49.7855\n",
      "Epoch 6/35, Training Loss: 42.7298\n",
      "Epoch 7/35, Training Loss: 36.6755\n",
      "Epoch 8/35, Training Loss: 31.6369\n",
      "Epoch 9/35, Training Loss: 27.6749\n",
      "Epoch 10/35, Training Loss: 24.6664\n",
      "Epoch 11/35, Training Loss: 22.5225\n",
      "Epoch 12/35, Training Loss: 21.0470\n",
      "Epoch 13/35, Training Loss: 20.2548\n",
      "Epoch 14/35, Training Loss: 20.1290\n",
      "Epoch 15/35, Training Loss: 20.0374\n",
      "Epoch 16/35, Training Loss: 20.2366\n",
      "Epoch 17/35, Training Loss: 19.0230\n",
      "Epoch 18/35, Training Loss: 18.0323\n",
      "Epoch 19/35, Training Loss: 16.0658\n",
      "Epoch 20/35, Training Loss: 14.5354\n",
      "Epoch 21/35, Training Loss: 13.7763\n",
      "Epoch 22/35, Training Loss: 12.9861\n",
      "Epoch 23/35, Training Loss: 12.5217\n",
      "Epoch 24/35, Training Loss: 12.1460\n",
      "Epoch 25/35, Training Loss: 11.6667\n",
      "Epoch 26/35, Training Loss: 11.3738\n",
      "Epoch 27/35, Training Loss: 11.1511\n",
      "Epoch 28/35, Training Loss: 10.9060\n",
      "Epoch 29/35, Training Loss: 10.7077\n",
      "Epoch 30/35, Training Loss: 10.5131\n",
      "Epoch 31/35, Training Loss: 10.3830\n",
      "Epoch 32/35, Training Loss: 10.2660\n",
      "Epoch 33/35, Training Loss: 10.1644\n",
      "Epoch 34/35, Training Loss: 10.1378\n",
      "Epoch 35/35, Training Loss: 10.0320\n",
      "Validation Loss: 11.1680\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 70.6696\n",
      "Epoch 2/35, Training Loss: 33.1778\n",
      "Epoch 3/35, Training Loss: 27.0254\n",
      "Epoch 4/35, Training Loss: 33.0930\n",
      "Epoch 5/35, Training Loss: 15.0419\n",
      "Epoch 6/35, Training Loss: 14.4971\n",
      "Epoch 7/35, Training Loss: 13.5838\n",
      "Epoch 8/35, Training Loss: 13.8825\n",
      "Epoch 9/35, Training Loss: 13.3893\n",
      "Epoch 10/35, Training Loss: 13.1906\n",
      "Epoch 11/35, Training Loss: 13.1346\n",
      "Epoch 12/35, Training Loss: 12.7752\n",
      "Epoch 13/35, Training Loss: 12.6884\n",
      "Epoch 14/35, Training Loss: 12.5546\n",
      "Epoch 15/35, Training Loss: 12.2249\n",
      "Epoch 16/35, Training Loss: 12.2377\n",
      "Epoch 17/35, Training Loss: 11.9864\n",
      "Epoch 18/35, Training Loss: 11.7350\n",
      "Epoch 19/35, Training Loss: 11.7866\n",
      "Epoch 20/35, Training Loss: 11.3768\n",
      "Epoch 21/35, Training Loss: 11.3384\n",
      "Epoch 22/35, Training Loss: 11.2501\n",
      "Epoch 23/35, Training Loss: 10.8344\n",
      "Epoch 24/35, Training Loss: 10.9983\n",
      "Epoch 25/35, Training Loss: 10.6586\n",
      "Epoch 26/35, Training Loss: 10.4644\n",
      "Epoch 27/35, Training Loss: 10.5857\n",
      "Epoch 28/35, Training Loss: 10.1321\n",
      "Epoch 29/35, Training Loss: 10.2442\n",
      "Epoch 30/35, Training Loss: 10.0535\n",
      "Epoch 31/35, Training Loss: 9.8118\n",
      "Epoch 32/35, Training Loss: 9.9322\n",
      "Epoch 33/35, Training Loss: 9.6470\n",
      "Epoch 34/35, Training Loss: 9.5630\n",
      "Epoch 35/35, Training Loss: 9.5502\n",
      "Validation Loss: 10.3668\n",
      "\n",
      "EnsembleForecaster CV Loss: 11.1336\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_7_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_7_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_7_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_7_CV3.png\n",
      "\n",
      "==========================\n",
      "Forecast Horizon (T_out): 14\n",
      "==========================\n",
      "\n",
      "Dataset loaded. Total samples: 3604\n",
      "\n",
      "--- CV Split 1 ---\n",
      "\n",
      "Training APDTFlow model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yotam\\code_projects\\APDTFlow\\venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Training Loss: 50.7187\n",
      "Epoch 2/35, Training Loss: 7.1772\n",
      "Epoch 3/35, Training Loss: 4.5103\n",
      "Epoch 4/35, Training Loss: 3.8838\n",
      "Epoch 5/35, Training Loss: 3.6378\n",
      "Epoch 6/35, Training Loss: 3.5009\n",
      "Epoch 7/35, Training Loss: 3.4066\n",
      "Epoch 8/35, Training Loss: 3.3212\n",
      "Epoch 9/35, Training Loss: 3.2524\n",
      "Epoch 10/35, Training Loss: 3.1911\n",
      "Epoch 11/35, Training Loss: 3.1367\n",
      "Epoch 12/35, Training Loss: 3.0872\n",
      "Epoch 13/35, Training Loss: 3.0434\n",
      "Epoch 14/35, Training Loss: 3.0017\n",
      "Epoch 15/35, Training Loss: 2.9689\n",
      "Epoch 16/35, Training Loss: 2.9366\n",
      "Epoch 17/35, Training Loss: 2.9058\n",
      "Epoch 18/35, Training Loss: 2.8766\n",
      "Epoch 19/35, Training Loss: 2.8518\n",
      "Epoch 20/35, Training Loss: 2.8254\n",
      "Epoch 21/35, Training Loss: 2.7986\n",
      "Epoch 22/35, Training Loss: 2.7777\n",
      "Epoch 23/35, Training Loss: 2.7576\n",
      "Epoch 24/35, Training Loss: 2.7354\n",
      "Epoch 25/35, Training Loss: 2.7148\n",
      "Epoch 26/35, Training Loss: 2.6932\n",
      "Epoch 27/35, Training Loss: 2.6724\n",
      "Epoch 28/35, Training Loss: 2.6508\n",
      "Epoch 29/35, Training Loss: 2.6295\n",
      "Epoch 30/35, Training Loss: 2.6077\n",
      "Epoch 31/35, Training Loss: 2.5864\n",
      "Epoch 32/35, Training Loss: 2.5637\n",
      "Epoch 33/35, Training Loss: 2.5418\n",
      "Epoch 34/35, Training Loss: 2.5144\n",
      "Epoch 35/35, Training Loss: 2.4913\n",
      "Validation Loss: 2.4810\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 109.4826\n",
      "Epoch 2/35, Training Loss: 81.8645\n",
      "Epoch 3/35, Training Loss: 71.7541\n",
      "Epoch 4/35, Training Loss: 63.1459\n",
      "Epoch 5/35, Training Loss: 55.1646\n",
      "Epoch 6/35, Training Loss: 47.8978\n",
      "Epoch 7/35, Training Loss: 41.4194\n",
      "Epoch 8/35, Training Loss: 35.8492\n",
      "Epoch 9/35, Training Loss: 31.2322\n",
      "Epoch 10/35, Training Loss: 27.5491\n",
      "Epoch 11/35, Training Loss: 24.7442\n",
      "Epoch 12/35, Training Loss: 22.6906\n",
      "Epoch 13/35, Training Loss: 21.2743\n",
      "Epoch 14/35, Training Loss: 20.4029\n",
      "Epoch 15/35, Training Loss: 19.9722\n",
      "Epoch 16/35, Training Loss: 19.7042\n",
      "Epoch 17/35, Training Loss: 18.5111\n",
      "Epoch 18/35, Training Loss: 16.0057\n",
      "Epoch 19/35, Training Loss: 15.0555\n",
      "Epoch 20/35, Training Loss: 13.3439\n",
      "Epoch 21/35, Training Loss: 12.8321\n",
      "Epoch 22/35, Training Loss: 12.0476\n",
      "Epoch 23/35, Training Loss: 11.6055\n",
      "Epoch 24/35, Training Loss: 11.3203\n",
      "Epoch 25/35, Training Loss: 11.0012\n",
      "Epoch 26/35, Training Loss: 10.8105\n",
      "Epoch 27/35, Training Loss: 10.5571\n",
      "Epoch 28/35, Training Loss: 10.4395\n",
      "Epoch 29/35, Training Loss: 10.2800\n",
      "Epoch 30/35, Training Loss: 10.1479\n",
      "Epoch 31/35, Training Loss: 9.9898\n",
      "Epoch 32/35, Training Loss: 9.9434\n",
      "Epoch 33/35, Training Loss: 9.8215\n",
      "Epoch 34/35, Training Loss: 9.7498\n",
      "Epoch 35/35, Training Loss: 9.6932\n",
      "Validation Loss: 12.4987\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 87.0596\n",
      "Epoch 2/35, Training Loss: 27.6247\n",
      "Epoch 3/35, Training Loss: 27.0407\n",
      "Epoch 4/35, Training Loss: 14.3548\n",
      "Epoch 5/35, Training Loss: 25.8232\n",
      "Epoch 6/35, Training Loss: 18.2103\n",
      "Epoch 7/35, Training Loss: 17.0705\n",
      "Epoch 8/35, Training Loss: 14.4245\n",
      "Epoch 9/35, Training Loss: 13.4549\n",
      "Epoch 10/35, Training Loss: 13.3721\n",
      "Epoch 11/35, Training Loss: 13.0973\n",
      "Epoch 12/35, Training Loss: 12.8403\n",
      "Epoch 13/35, Training Loss: 12.8732\n",
      "Epoch 14/35, Training Loss: 12.5336\n",
      "Epoch 15/35, Training Loss: 12.3750\n",
      "Epoch 16/35, Training Loss: 12.4338\n",
      "Epoch 17/35, Training Loss: 12.0598\n",
      "Epoch 18/35, Training Loss: 11.9680\n",
      "Epoch 19/35, Training Loss: 12.0139\n",
      "Epoch 20/35, Training Loss: 11.6925\n",
      "Epoch 21/35, Training Loss: 11.7041\n",
      "Epoch 22/35, Training Loss: 11.6770\n",
      "Epoch 23/35, Training Loss: 11.4359\n",
      "Epoch 24/35, Training Loss: 11.4591\n",
      "Epoch 25/35, Training Loss: 11.3834\n",
      "Epoch 26/35, Training Loss: 11.2589\n",
      "Epoch 27/35, Training Loss: 11.2343\n",
      "Epoch 28/35, Training Loss: 11.1822\n",
      "Epoch 29/35, Training Loss: 11.0599\n",
      "Epoch 30/35, Training Loss: 11.0241\n",
      "Epoch 31/35, Training Loss: 10.9632\n",
      "Epoch 32/35, Training Loss: 11.1860\n",
      "Epoch 33/35, Training Loss: 11.2138\n",
      "Epoch 34/35, Training Loss: 10.4279\n",
      "Epoch 35/35, Training Loss: 10.8811\n",
      "Validation Loss: 13.2506\n",
      "\n",
      "EnsembleForecaster CV Loss: 13.0179\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_14_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_14_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_14_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_14_CV1.png\n",
      "\n",
      "--- CV Split 2 ---\n",
      "\n",
      "Training APDTFlow model...\n",
      "Epoch 1/35, Training Loss: 17.5810\n",
      "Epoch 2/35, Training Loss: 4.9230\n",
      "Epoch 3/35, Training Loss: 3.9281\n",
      "Epoch 4/35, Training Loss: 3.5834\n",
      "Epoch 5/35, Training Loss: 3.3834\n",
      "Epoch 6/35, Training Loss: 3.2490\n",
      "Epoch 7/35, Training Loss: 3.1481\n",
      "Epoch 8/35, Training Loss: 3.0708\n",
      "Epoch 9/35, Training Loss: 3.0135\n",
      "Epoch 10/35, Training Loss: 2.9654\n",
      "Epoch 11/35, Training Loss: 2.9269\n",
      "Epoch 12/35, Training Loss: 2.8975\n",
      "Epoch 13/35, Training Loss: 2.8685\n",
      "Epoch 14/35, Training Loss: 2.8459\n",
      "Epoch 15/35, Training Loss: 2.8239\n",
      "Epoch 16/35, Training Loss: 2.8020\n",
      "Epoch 17/35, Training Loss: 2.7821\n",
      "Epoch 18/35, Training Loss: 2.7602\n",
      "Epoch 19/35, Training Loss: 2.7399\n",
      "Epoch 20/35, Training Loss: 2.7183\n",
      "Epoch 21/35, Training Loss: 2.6970\n",
      "Epoch 22/35, Training Loss: 2.6732\n",
      "Epoch 23/35, Training Loss: 2.6495\n",
      "Epoch 24/35, Training Loss: 2.6265\n",
      "Epoch 25/35, Training Loss: 2.6001\n",
      "Epoch 26/35, Training Loss: 2.5739\n",
      "Epoch 27/35, Training Loss: 2.5467\n",
      "Epoch 28/35, Training Loss: 2.5188\n",
      "Epoch 29/35, Training Loss: 2.4878\n",
      "Epoch 30/35, Training Loss: 2.4543\n",
      "Epoch 31/35, Training Loss: 2.4231\n",
      "Epoch 32/35, Training Loss: 2.3866\n",
      "Epoch 33/35, Training Loss: 2.3474\n",
      "Epoch 34/35, Training Loss: 2.3089\n",
      "Epoch 35/35, Training Loss: 2.2730\n",
      "Validation Loss: 1.9912\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 100.3679\n",
      "Epoch 2/35, Training Loss: 75.5149\n",
      "Epoch 3/35, Training Loss: 66.1271\n",
      "Epoch 4/35, Training Loss: 57.7784\n",
      "Epoch 5/35, Training Loss: 49.9388\n",
      "Epoch 6/35, Training Loss: 42.7480\n",
      "Epoch 7/35, Training Loss: 36.4375\n",
      "Epoch 8/35, Training Loss: 31.1326\n",
      "Epoch 9/35, Training Loss: 26.8717\n",
      "Epoch 10/35, Training Loss: 23.6175\n",
      "Epoch 11/35, Training Loss: 21.2622\n",
      "Epoch 12/35, Training Loss: 19.6431\n",
      "Epoch 13/35, Training Loss: 18.6020\n",
      "Epoch 14/35, Training Loss: 18.1003\n",
      "Epoch 15/35, Training Loss: 17.9206\n",
      "Epoch 16/35, Training Loss: 16.9403\n",
      "Epoch 17/35, Training Loss: 15.0734\n",
      "Epoch 18/35, Training Loss: 13.4517\n",
      "Epoch 19/35, Training Loss: 12.9101\n",
      "Epoch 20/35, Training Loss: 11.9605\n",
      "Epoch 21/35, Training Loss: 11.7316\n",
      "Epoch 22/35, Training Loss: 11.3330\n",
      "Epoch 23/35, Training Loss: 11.1237\n",
      "Epoch 24/35, Training Loss: 10.8786\n",
      "Epoch 25/35, Training Loss: 10.7248\n",
      "Epoch 26/35, Training Loss: 10.5996\n",
      "Epoch 27/35, Training Loss: 10.4473\n",
      "Epoch 28/35, Training Loss: 10.4028\n",
      "Epoch 29/35, Training Loss: 10.2232\n",
      "Epoch 30/35, Training Loss: 10.2084\n",
      "Epoch 31/35, Training Loss: 10.0884\n",
      "Epoch 32/35, Training Loss: 9.9721\n",
      "Epoch 33/35, Training Loss: 9.8551\n",
      "Epoch 34/35, Training Loss: 9.8681\n",
      "Epoch 35/35, Training Loss: 9.8038\n",
      "Validation Loss: 14.2150\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 84.7972\n",
      "Epoch 2/35, Training Loss: 22.5831\n",
      "Epoch 3/35, Training Loss: 26.0273\n",
      "Epoch 4/35, Training Loss: 13.6509\n",
      "Epoch 5/35, Training Loss: 14.2364\n",
      "Epoch 6/35, Training Loss: 12.7862\n",
      "Epoch 7/35, Training Loss: 12.1406\n",
      "Epoch 8/35, Training Loss: 12.2583\n",
      "Epoch 9/35, Training Loss: 11.9439\n",
      "Epoch 10/35, Training Loss: 11.8458\n",
      "Epoch 11/35, Training Loss: 11.8162\n",
      "Epoch 12/35, Training Loss: 11.6754\n",
      "Epoch 13/35, Training Loss: 11.6126\n",
      "Epoch 14/35, Training Loss: 11.6085\n",
      "Epoch 15/35, Training Loss: 11.4309\n",
      "Epoch 16/35, Training Loss: 11.4098\n",
      "Epoch 17/35, Training Loss: 11.3957\n",
      "Epoch 18/35, Training Loss: 11.2458\n",
      "Epoch 19/35, Training Loss: 11.2262\n",
      "Epoch 20/35, Training Loss: 11.1814\n",
      "Epoch 21/35, Training Loss: 11.0610\n",
      "Epoch 22/35, Training Loss: 11.0027\n",
      "Epoch 23/35, Training Loss: 10.9683\n",
      "Epoch 24/35, Training Loss: 10.8554\n",
      "Epoch 25/35, Training Loss: 10.7866\n",
      "Epoch 26/35, Training Loss: 10.7701\n",
      "Epoch 27/35, Training Loss: 10.6615\n",
      "Epoch 28/35, Training Loss: 10.5540\n",
      "Epoch 29/35, Training Loss: 10.5636\n",
      "Epoch 30/35, Training Loss: 10.4457\n",
      "Epoch 31/35, Training Loss: 10.3878\n",
      "Epoch 32/35, Training Loss: 10.3840\n",
      "Epoch 33/35, Training Loss: 10.2400\n",
      "Epoch 34/35, Training Loss: 10.2179\n",
      "Epoch 35/35, Training Loss: 10.1934\n",
      "Validation Loss: 17.5132\n",
      "\n",
      "EnsembleForecaster CV Loss: 13.1941\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_14_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_14_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_14_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_14_CV2.png\n",
      "\n",
      "--- CV Split 3 ---\n",
      "\n",
      "Training APDTFlow model...\n",
      "Epoch 1/35, Training Loss: 29.1099\n",
      "Epoch 2/35, Training Loss: 5.1581\n",
      "Epoch 3/35, Training Loss: 4.0300\n",
      "Epoch 4/35, Training Loss: 3.6057\n",
      "Epoch 5/35, Training Loss: 3.3766\n",
      "Epoch 6/35, Training Loss: 3.2296\n",
      "Epoch 7/35, Training Loss: 3.1212\n",
      "Epoch 8/35, Training Loss: 3.0408\n",
      "Epoch 9/35, Training Loss: 2.9814\n",
      "Epoch 10/35, Training Loss: 2.9296\n",
      "Epoch 11/35, Training Loss: 2.8865\n",
      "Epoch 12/35, Training Loss: 2.8501\n",
      "Epoch 13/35, Training Loss: 2.8204\n",
      "Epoch 14/35, Training Loss: 2.7902\n",
      "Epoch 15/35, Training Loss: 2.7671\n",
      "Epoch 16/35, Training Loss: 2.7416\n",
      "Epoch 17/35, Training Loss: 2.7180\n",
      "Epoch 18/35, Training Loss: 2.6976\n",
      "Epoch 19/35, Training Loss: 2.6747\n",
      "Epoch 20/35, Training Loss: 2.6568\n",
      "Epoch 21/35, Training Loss: 2.6339\n",
      "Epoch 22/35, Training Loss: 2.6141\n",
      "Epoch 23/35, Training Loss: 2.5900\n",
      "Epoch 24/35, Training Loss: 2.5701\n",
      "Epoch 25/35, Training Loss: 2.5473\n",
      "Epoch 26/35, Training Loss: 2.5264\n",
      "Epoch 27/35, Training Loss: 2.5011\n",
      "Epoch 28/35, Training Loss: 2.4746\n",
      "Epoch 29/35, Training Loss: 2.4526\n",
      "Epoch 30/35, Training Loss: 2.4289\n",
      "Epoch 31/35, Training Loss: 2.3991\n",
      "Epoch 32/35, Training Loss: 2.3732\n",
      "Epoch 33/35, Training Loss: 2.3449\n",
      "Epoch 34/35, Training Loss: 2.3209\n",
      "Epoch 35/35, Training Loss: 2.2912\n",
      "Validation Loss: 2.0032\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 96.9927\n",
      "Epoch 2/35, Training Loss: 78.3533\n",
      "Epoch 3/35, Training Loss: 69.1688\n",
      "Epoch 4/35, Training Loss: 60.7176\n",
      "Epoch 5/35, Training Loss: 52.7193\n",
      "Epoch 6/35, Training Loss: 45.3807\n",
      "Epoch 7/35, Training Loss: 38.9120\n",
      "Epoch 8/35, Training Loss: 33.4605\n",
      "Epoch 9/35, Training Loss: 29.0467\n",
      "Epoch 10/35, Training Loss: 25.6708\n",
      "Epoch 11/35, Training Loss: 23.1899\n",
      "Epoch 12/35, Training Loss: 21.4710\n",
      "Epoch 13/35, Training Loss: 20.4011\n",
      "Epoch 14/35, Training Loss: 20.0266\n",
      "Epoch 15/35, Training Loss: 20.1533\n",
      "Epoch 16/35, Training Loss: 20.0974\n",
      "Epoch 17/35, Training Loss: 19.4018\n",
      "Epoch 18/35, Training Loss: 18.2899\n",
      "Epoch 19/35, Training Loss: 16.7108\n",
      "Epoch 20/35, Training Loss: 15.2895\n",
      "Epoch 21/35, Training Loss: 14.3825\n",
      "Epoch 22/35, Training Loss: 13.5530\n",
      "Epoch 23/35, Training Loss: 12.8716\n",
      "Epoch 24/35, Training Loss: 12.3928\n",
      "Epoch 25/35, Training Loss: 11.9930\n",
      "Epoch 26/35, Training Loss: 11.6793\n",
      "Epoch 27/35, Training Loss: 11.3597\n",
      "Epoch 28/35, Training Loss: 11.2300\n",
      "Epoch 29/35, Training Loss: 10.9910\n",
      "Epoch 30/35, Training Loss: 10.8832\n",
      "Epoch 31/35, Training Loss: 10.7385\n",
      "Epoch 32/35, Training Loss: 10.5826\n",
      "Epoch 33/35, Training Loss: 10.5122\n",
      "Epoch 34/35, Training Loss: 10.3805\n",
      "Epoch 35/35, Training Loss: 10.3293\n",
      "Validation Loss: 11.1384\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 122.8582\n",
      "Epoch 2/35, Training Loss: 21.8755\n",
      "Epoch 3/35, Training Loss: 42.6694\n",
      "Epoch 4/35, Training Loss: 15.5014\n",
      "Epoch 5/35, Training Loss: 14.5811\n",
      "Epoch 6/35, Training Loss: 13.9785\n",
      "Epoch 7/35, Training Loss: 13.9497\n",
      "Epoch 8/35, Training Loss: 13.6227\n",
      "Epoch 9/35, Training Loss: 13.3790\n",
      "Epoch 10/35, Training Loss: 13.2942\n",
      "Epoch 11/35, Training Loss: 13.0115\n",
      "Epoch 12/35, Training Loss: 12.9072\n",
      "Epoch 13/35, Training Loss: 12.7931\n",
      "Epoch 14/35, Training Loss: 12.5940\n",
      "Epoch 15/35, Training Loss: 12.5497\n",
      "Epoch 16/35, Training Loss: 12.4408\n",
      "Epoch 17/35, Training Loss: 12.2714\n",
      "Epoch 18/35, Training Loss: 12.2602\n",
      "Epoch 19/35, Training Loss: 12.1225\n",
      "Epoch 20/35, Training Loss: 12.0589\n",
      "Epoch 21/35, Training Loss: 12.0070\n",
      "Epoch 22/35, Training Loss: 11.8628\n",
      "Epoch 23/35, Training Loss: 11.8416\n",
      "Epoch 24/35, Training Loss: 11.7243\n",
      "Epoch 25/35, Training Loss: 11.6008\n",
      "Epoch 26/35, Training Loss: 11.5843\n",
      "Epoch 27/35, Training Loss: 11.4212\n",
      "Epoch 28/35, Training Loss: 11.3632\n",
      "Epoch 29/35, Training Loss: 11.2956\n",
      "Epoch 30/35, Training Loss: 11.1480\n",
      "Epoch 31/35, Training Loss: 11.1375\n",
      "Epoch 32/35, Training Loss: 11.0124\n",
      "Epoch 33/35, Training Loss: 10.9181\n",
      "Epoch 34/35, Training Loss: 10.9166\n",
      "Epoch 35/35, Training Loss: 10.7253\n",
      "Validation Loss: 8.4910\n",
      "\n",
      "EnsembleForecaster CV Loss: 9.7695\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_14_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_14_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_14_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_14_CV3.png\n",
      "\n",
      "==========================\n",
      "Forecast Horizon (T_out): 30\n",
      "==========================\n",
      "\n",
      "Dataset loaded. Total samples: 3588\n",
      "\n",
      "--- CV Split 1 ---\n",
      "\n",
      "Training APDTFlow model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yotam\\code_projects\\APDTFlow\\venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Training Loss: 10.6770\n",
      "Epoch 2/35, Training Loss: 4.1604\n",
      "Epoch 3/35, Training Loss: 3.4915\n",
      "Epoch 4/35, Training Loss: 3.2187\n",
      "Epoch 5/35, Training Loss: 3.0554\n",
      "Epoch 6/35, Training Loss: 2.9505\n",
      "Epoch 7/35, Training Loss: 2.8829\n",
      "Epoch 8/35, Training Loss: 2.8309\n",
      "Epoch 9/35, Training Loss: 2.7900\n",
      "Epoch 10/35, Training Loss: 2.7502\n",
      "Epoch 11/35, Training Loss: 2.7143\n",
      "Epoch 12/35, Training Loss: 2.6748\n",
      "Epoch 13/35, Training Loss: 2.6364\n",
      "Epoch 14/35, Training Loss: 2.5963\n",
      "Epoch 15/35, Training Loss: 2.5543\n",
      "Epoch 16/35, Training Loss: 2.5111\n",
      "Epoch 17/35, Training Loss: 2.4650\n",
      "Epoch 18/35, Training Loss: 2.4179\n",
      "Epoch 19/35, Training Loss: 2.3724\n",
      "Epoch 20/35, Training Loss: 2.3229\n",
      "Epoch 21/35, Training Loss: 2.2771\n",
      "Epoch 22/35, Training Loss: 2.2308\n",
      "Epoch 23/35, Training Loss: 2.1876\n",
      "Epoch 24/35, Training Loss: 2.1469\n",
      "Epoch 25/35, Training Loss: 2.1091\n",
      "Epoch 26/35, Training Loss: 2.0757\n",
      "Epoch 27/35, Training Loss: 2.0493\n",
      "Epoch 28/35, Training Loss: 2.0251\n",
      "Epoch 29/35, Training Loss: 2.0061\n",
      "Epoch 30/35, Training Loss: 1.9936\n",
      "Epoch 31/35, Training Loss: 1.9828\n",
      "Epoch 32/35, Training Loss: 1.9764\n",
      "Epoch 33/35, Training Loss: 1.9738\n",
      "Epoch 34/35, Training Loss: 1.9692\n",
      "Epoch 35/35, Training Loss: 1.9667\n",
      "Validation Loss: 1.7960\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 114.9690\n",
      "Epoch 2/35, Training Loss: 82.6593\n",
      "Epoch 3/35, Training Loss: 72.1550\n",
      "Epoch 4/35, Training Loss: 63.2317\n",
      "Epoch 5/35, Training Loss: 54.8881\n",
      "Epoch 6/35, Training Loss: 47.2693\n",
      "Epoch 7/35, Training Loss: 40.5320\n",
      "Epoch 8/35, Training Loss: 34.7951\n",
      "Epoch 9/35, Training Loss: 30.1237\n",
      "Epoch 10/35, Training Loss: 26.4670\n",
      "Epoch 11/35, Training Loss: 23.7343\n",
      "Epoch 12/35, Training Loss: 21.7917\n",
      "Epoch 13/35, Training Loss: 20.4708\n",
      "Epoch 14/35, Training Loss: 19.6836\n",
      "Epoch 15/35, Training Loss: 19.4118\n",
      "Epoch 16/35, Training Loss: 19.2616\n",
      "Epoch 17/35, Training Loss: 19.0585\n",
      "Epoch 18/35, Training Loss: 18.6385\n",
      "Epoch 19/35, Training Loss: 17.5181\n",
      "Epoch 20/35, Training Loss: 15.5767\n",
      "Epoch 21/35, Training Loss: 14.2729\n",
      "Epoch 22/35, Training Loss: 13.6469\n",
      "Epoch 23/35, Training Loss: 12.5547\n",
      "Epoch 24/35, Training Loss: 12.5567\n",
      "Epoch 25/35, Training Loss: 11.7245\n",
      "Epoch 26/35, Training Loss: 11.8400\n",
      "Epoch 27/35, Training Loss: 11.2654\n",
      "Epoch 28/35, Training Loss: 11.3827\n",
      "Epoch 29/35, Training Loss: 10.9549\n",
      "Epoch 30/35, Training Loss: 11.0541\n",
      "Epoch 31/35, Training Loss: 10.7543\n",
      "Epoch 32/35, Training Loss: 10.8292\n",
      "Epoch 33/35, Training Loss: 10.5312\n",
      "Epoch 34/35, Training Loss: 10.5628\n",
      "Epoch 35/35, Training Loss: 10.4167\n",
      "Validation Loss: 14.8438\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 85.7221\n",
      "Epoch 2/35, Training Loss: 21.2966\n",
      "Epoch 3/35, Training Loss: 19.8822\n",
      "Epoch 4/35, Training Loss: 15.8787\n",
      "Epoch 5/35, Training Loss: 16.7909\n",
      "Epoch 6/35, Training Loss: 17.1919\n",
      "Epoch 7/35, Training Loss: 13.0705\n",
      "Epoch 8/35, Training Loss: 15.5885\n",
      "Epoch 9/35, Training Loss: 15.4664\n",
      "Epoch 10/35, Training Loss: 12.2143\n",
      "Epoch 11/35, Training Loss: 14.8192\n",
      "Epoch 12/35, Training Loss: 13.8446\n",
      "Epoch 13/35, Training Loss: 12.0274\n",
      "Epoch 14/35, Training Loss: 13.9501\n",
      "Epoch 15/35, Training Loss: 12.6477\n",
      "Epoch 16/35, Training Loss: 12.0696\n",
      "Epoch 17/35, Training Loss: 13.2759\n",
      "Epoch 18/35, Training Loss: 12.0309\n",
      "Epoch 19/35, Training Loss: 12.0284\n",
      "Epoch 20/35, Training Loss: 12.6349\n",
      "Epoch 21/35, Training Loss: 11.7115\n",
      "Epoch 22/35, Training Loss: 11.9092\n",
      "Epoch 23/35, Training Loss: 12.1322\n",
      "Epoch 24/35, Training Loss: 11.4648\n",
      "Epoch 25/35, Training Loss: 11.6592\n",
      "Epoch 26/35, Training Loss: 11.6738\n",
      "Epoch 27/35, Training Loss: 11.2965\n",
      "Epoch 28/35, Training Loss: 11.3973\n",
      "Epoch 29/35, Training Loss: 11.3465\n",
      "Epoch 30/35, Training Loss: 11.0876\n",
      "Epoch 31/35, Training Loss: 11.2469\n",
      "Epoch 32/35, Training Loss: 11.1040\n",
      "Epoch 33/35, Training Loss: 10.8783\n",
      "Epoch 34/35, Training Loss: 11.0092\n",
      "Epoch 35/35, Training Loss: 10.8094\n",
      "Validation Loss: 15.3171\n",
      "\n",
      "EnsembleForecaster CV Loss: 13.0490\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_30_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_30_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_30_CV1.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_30_CV1.png\n",
      "\n",
      "--- CV Split 2 ---\n",
      "\n",
      "Training APDTFlow model...\n",
      "Epoch 1/35, Training Loss: 44.8613\n",
      "Epoch 2/35, Training Loss: 6.2954\n",
      "Epoch 3/35, Training Loss: 4.4186\n",
      "Epoch 4/35, Training Loss: 3.8712\n",
      "Epoch 5/35, Training Loss: 3.6117\n",
      "Epoch 6/35, Training Loss: 3.4578\n",
      "Epoch 7/35, Training Loss: 3.3479\n",
      "Epoch 8/35, Training Loss: 3.2655\n",
      "Epoch 9/35, Training Loss: 3.1980\n",
      "Epoch 10/35, Training Loss: 3.1425\n",
      "Epoch 11/35, Training Loss: 3.0913\n",
      "Epoch 12/35, Training Loss: 3.0458\n",
      "Epoch 13/35, Training Loss: 3.0052\n",
      "Epoch 14/35, Training Loss: 2.9696\n",
      "Epoch 15/35, Training Loss: 2.9363\n",
      "Epoch 16/35, Training Loss: 2.9059\n",
      "Epoch 17/35, Training Loss: 2.8791\n",
      "Epoch 18/35, Training Loss: 2.8544\n",
      "Epoch 19/35, Training Loss: 2.8318\n",
      "Epoch 20/35, Training Loss: 2.8087\n",
      "Epoch 21/35, Training Loss: 2.7874\n",
      "Epoch 22/35, Training Loss: 2.7656\n",
      "Epoch 23/35, Training Loss: 2.7450\n",
      "Epoch 24/35, Training Loss: 2.7244\n",
      "Epoch 25/35, Training Loss: 2.7036\n",
      "Epoch 26/35, Training Loss: 2.6813\n",
      "Epoch 27/35, Training Loss: 2.6605\n",
      "Epoch 28/35, Training Loss: 2.6374\n",
      "Epoch 29/35, Training Loss: 2.6126\n",
      "Epoch 30/35, Training Loss: 2.5888\n",
      "Epoch 31/35, Training Loss: 2.5618\n",
      "Epoch 32/35, Training Loss: 2.5361\n",
      "Epoch 33/35, Training Loss: 2.5068\n",
      "Epoch 34/35, Training Loss: 2.4775\n",
      "Epoch 35/35, Training Loss: 2.4469\n",
      "Validation Loss: 2.0955\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 108.5842\n",
      "Epoch 2/35, Training Loss: 73.6219\n",
      "Epoch 3/35, Training Loss: 63.8878\n",
      "Epoch 4/35, Training Loss: 55.4372\n",
      "Epoch 5/35, Training Loss: 47.7479\n",
      "Epoch 6/35, Training Loss: 40.9064\n",
      "Epoch 7/35, Training Loss: 35.0118\n",
      "Epoch 8/35, Training Loss: 30.1014\n",
      "Epoch 9/35, Training Loss: 26.1937\n",
      "Epoch 10/35, Training Loss: 23.1954\n",
      "Epoch 11/35, Training Loss: 21.0148\n",
      "Epoch 12/35, Training Loss: 19.4851\n",
      "Epoch 13/35, Training Loss: 18.4775\n",
      "Epoch 14/35, Training Loss: 17.9052\n",
      "Epoch 15/35, Training Loss: 17.7937\n",
      "Epoch 16/35, Training Loss: 17.7627\n",
      "Epoch 17/35, Training Loss: 16.8405\n",
      "Epoch 18/35, Training Loss: 15.5972\n",
      "Epoch 19/35, Training Loss: 14.4735\n",
      "Epoch 20/35, Training Loss: 13.7260\n",
      "Epoch 21/35, Training Loss: 13.1092\n",
      "Epoch 22/35, Training Loss: 12.7641\n",
      "Epoch 23/35, Training Loss: 12.3827\n",
      "Epoch 24/35, Training Loss: 12.2139\n",
      "Epoch 25/35, Training Loss: 12.0230\n",
      "Epoch 26/35, Training Loss: 11.9013\n",
      "Epoch 27/35, Training Loss: 11.7539\n",
      "Epoch 28/35, Training Loss: 11.6835\n",
      "Epoch 29/35, Training Loss: 11.5637\n",
      "Epoch 30/35, Training Loss: 11.4649\n",
      "Epoch 31/35, Training Loss: 11.3653\n",
      "Epoch 32/35, Training Loss: 11.2831\n",
      "Epoch 33/35, Training Loss: 11.2373\n",
      "Epoch 34/35, Training Loss: 11.1783\n",
      "Epoch 35/35, Training Loss: 11.0539\n",
      "Validation Loss: 18.0998\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 47.4037\n",
      "Epoch 2/35, Training Loss: 91.5391\n",
      "Epoch 3/35, Training Loss: 26.3126\n",
      "Epoch 4/35, Training Loss: 17.2950\n",
      "Epoch 5/35, Training Loss: 16.7251\n",
      "Epoch 6/35, Training Loss: 15.9465\n",
      "Epoch 7/35, Training Loss: 15.8793\n",
      "Epoch 8/35, Training Loss: 15.8153\n",
      "Epoch 9/35, Training Loss: 15.5067\n",
      "Epoch 10/35, Training Loss: 15.5468\n",
      "Epoch 11/35, Training Loss: 15.3634\n",
      "Epoch 12/35, Training Loss: 15.2078\n",
      "Epoch 13/35, Training Loss: 15.1701\n",
      "Epoch 14/35, Training Loss: 14.9941\n",
      "Epoch 15/35, Training Loss: 14.8909\n",
      "Epoch 16/35, Training Loss: 14.8250\n",
      "Epoch 17/35, Training Loss: 14.6642\n",
      "Epoch 18/35, Training Loss: 14.6078\n",
      "Epoch 19/35, Training Loss: 14.5053\n",
      "Epoch 20/35, Training Loss: 14.3633\n",
      "Epoch 21/35, Training Loss: 14.3235\n",
      "Epoch 22/35, Training Loss: 14.1866\n",
      "Epoch 23/35, Training Loss: 14.0524\n",
      "Epoch 24/35, Training Loss: 13.9991\n",
      "Epoch 25/35, Training Loss: 13.8589\n",
      "Epoch 26/35, Training Loss: 13.7695\n",
      "Epoch 27/35, Training Loss: 13.6946\n",
      "Epoch 28/35, Training Loss: 13.5544\n",
      "Epoch 29/35, Training Loss: 13.4668\n",
      "Epoch 30/35, Training Loss: 13.4053\n",
      "Epoch 31/35, Training Loss: 13.2611\n",
      "Epoch 32/35, Training Loss: 13.2147\n",
      "Epoch 33/35, Training Loss: 13.0990\n",
      "Epoch 34/35, Training Loss: 12.9860\n",
      "Epoch 35/35, Training Loss: 12.9156\n",
      "Validation Loss: 26.6294\n",
      "\n",
      "EnsembleForecaster CV Loss: 14.5650\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_30_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_30_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_30_CV2.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_30_CV2.png\n",
      "\n",
      "--- CV Split 3 ---\n",
      "\n",
      "Training APDTFlow model...\n",
      "Epoch 1/35, Training Loss: 18.4724\n",
      "Epoch 2/35, Training Loss: 4.4010\n",
      "Epoch 3/35, Training Loss: 3.6228\n",
      "Epoch 4/35, Training Loss: 3.3108\n",
      "Epoch 5/35, Training Loss: 3.1449\n",
      "Epoch 6/35, Training Loss: 3.0342\n",
      "Epoch 7/35, Training Loss: 2.9505\n",
      "Epoch 8/35, Training Loss: 2.8855\n",
      "Epoch 9/35, Training Loss: 2.8320\n",
      "Epoch 10/35, Training Loss: 2.7878\n",
      "Epoch 11/35, Training Loss: 2.7485\n",
      "Epoch 12/35, Training Loss: 2.7150\n",
      "Epoch 13/35, Training Loss: 2.6840\n",
      "Epoch 14/35, Training Loss: 2.6550\n",
      "Epoch 15/35, Training Loss: 2.6277\n",
      "Epoch 16/35, Training Loss: 2.6013\n",
      "Epoch 17/35, Training Loss: 2.5739\n",
      "Epoch 18/35, Training Loss: 2.5444\n",
      "Epoch 19/35, Training Loss: 2.5164\n",
      "Epoch 20/35, Training Loss: 2.4858\n",
      "Epoch 21/35, Training Loss: 2.4549\n",
      "Epoch 22/35, Training Loss: 2.4221\n",
      "Epoch 23/35, Training Loss: 2.3891\n",
      "Epoch 24/35, Training Loss: 2.3550\n",
      "Epoch 25/35, Training Loss: 2.3177\n",
      "Epoch 26/35, Training Loss: 2.2826\n",
      "Epoch 27/35, Training Loss: 2.2455\n",
      "Epoch 28/35, Training Loss: 2.2092\n",
      "Epoch 29/35, Training Loss: 2.1761\n",
      "Epoch 30/35, Training Loss: 2.1430\n",
      "Epoch 31/35, Training Loss: 2.1137\n",
      "Epoch 32/35, Training Loss: 2.0854\n",
      "Epoch 33/35, Training Loss: 2.0651\n",
      "Epoch 34/35, Training Loss: 2.0435\n",
      "Epoch 35/35, Training Loss: 2.0283\n",
      "Validation Loss: 2.1160\n",
      "\n",
      "Training TransformerForecaster model...\n",
      "Epoch 1/35, Training Loss: 87.7498\n",
      "Epoch 2/35, Training Loss: 68.8416\n",
      "Epoch 3/35, Training Loss: 60.1133\n",
      "Epoch 4/35, Training Loss: 52.4747\n",
      "Epoch 5/35, Training Loss: 45.4941\n",
      "Epoch 6/35, Training Loss: 39.2845\n",
      "Epoch 7/35, Training Loss: 34.0141\n",
      "Epoch 8/35, Training Loss: 29.7135\n",
      "Epoch 9/35, Training Loss: 26.3723\n",
      "Epoch 10/35, Training Loss: 23.9015\n",
      "Epoch 11/35, Training Loss: 22.1650\n",
      "Epoch 12/35, Training Loss: 21.0173\n",
      "Epoch 13/35, Training Loss: 20.4248\n",
      "Epoch 14/35, Training Loss: 20.5741\n",
      "Epoch 15/35, Training Loss: 21.0538\n",
      "Epoch 16/35, Training Loss: 19.4080\n",
      "Epoch 17/35, Training Loss: 17.8485\n",
      "Epoch 18/35, Training Loss: 16.7194\n",
      "Epoch 19/35, Training Loss: 15.5866\n",
      "Epoch 20/35, Training Loss: 14.7955\n",
      "Epoch 21/35, Training Loss: 14.1320\n",
      "Epoch 22/35, Training Loss: 13.7166\n",
      "Epoch 23/35, Training Loss: 13.2135\n",
      "Epoch 24/35, Training Loss: 12.9445\n",
      "Epoch 25/35, Training Loss: 12.5901\n",
      "Epoch 26/35, Training Loss: 12.4004\n",
      "Epoch 27/35, Training Loss: 12.1678\n",
      "Epoch 28/35, Training Loss: 12.0152\n",
      "Epoch 29/35, Training Loss: 11.8212\n",
      "Epoch 30/35, Training Loss: 11.7219\n",
      "Epoch 31/35, Training Loss: 11.5766\n",
      "Epoch 32/35, Training Loss: 11.5264\n",
      "Epoch 33/35, Training Loss: 11.4062\n",
      "Epoch 34/35, Training Loss: 11.3838\n",
      "Epoch 35/35, Training Loss: 11.2997\n",
      "Validation Loss: 9.8290\n",
      "\n",
      "Training TCNForecaster model...\n",
      "Epoch 1/35, Training Loss: 138.6121\n",
      "Epoch 2/35, Training Loss: 34.2588\n",
      "Epoch 3/35, Training Loss: 44.1224\n",
      "Epoch 4/35, Training Loss: 17.9978\n",
      "Epoch 5/35, Training Loss: 19.2965\n",
      "Epoch 6/35, Training Loss: 16.4801\n",
      "Epoch 7/35, Training Loss: 17.1673\n",
      "Epoch 8/35, Training Loss: 15.6789\n",
      "Epoch 9/35, Training Loss: 15.9164\n",
      "Epoch 10/35, Training Loss: 15.0761\n",
      "Epoch 11/35, Training Loss: 15.0416\n",
      "Epoch 12/35, Training Loss: 14.5279\n",
      "Epoch 13/35, Training Loss: 14.3933\n",
      "Epoch 14/35, Training Loss: 14.0490\n",
      "Epoch 15/35, Training Loss: 13.8970\n",
      "Epoch 16/35, Training Loss: 13.6474\n",
      "Epoch 17/35, Training Loss: 13.4876\n",
      "Epoch 18/35, Training Loss: 13.2957\n",
      "Epoch 19/35, Training Loss: 13.1461\n",
      "Epoch 20/35, Training Loss: 12.9934\n",
      "Epoch 21/35, Training Loss: 12.8711\n",
      "Epoch 22/35, Training Loss: 12.6913\n",
      "Epoch 23/35, Training Loss: 12.5874\n",
      "Epoch 24/35, Training Loss: 12.4367\n",
      "Epoch 25/35, Training Loss: 12.3275\n",
      "Epoch 26/35, Training Loss: 12.3557\n",
      "Epoch 27/35, Training Loss: 12.1649\n",
      "Epoch 28/35, Training Loss: 12.1635\n",
      "Epoch 29/35, Training Loss: 11.7447\n",
      "Epoch 30/35, Training Loss: 12.0158\n",
      "Epoch 31/35, Training Loss: 11.4653\n",
      "Epoch 32/35, Training Loss: 11.9275\n",
      "Epoch 33/35, Training Loss: 11.2599\n",
      "Epoch 34/35, Training Loss: 11.8507\n",
      "Epoch 35/35, Training Loss: 11.0194\n",
      "Validation Loss: 8.9168\n",
      "\n",
      "EnsembleForecaster CV Loss: 12.6217\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\APDTFlow_Forecast_Horizon_30_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Transformer_Forecast_Horizon_30_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\TCN_Forecast_Horizon_30_CV3.png\n",
      "Saved forecast plot to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Ensemble_Forecast_Horizon_30_CV3.png\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "max_cv_plots = 3\n",
    "\n",
    "for T_out in forecast_horizons:\n",
    "    print(\"\\n==========================\")\n",
    "    print(f\"Forecast Horizon (T_out): {T_out}\")\n",
    "    print(\"==========================\\n\")\n",
    "    dataset = TimeSeriesWindowDataset(\n",
    "        csv_file=clean_csv,\n",
    "        date_col=\"Date\",\n",
    "        value_col=\"Daily minimum temperatures\",\n",
    "        T_in=T_in,\n",
    "        T_out=T_out,\n",
    "        transform=normalize_tensor\n",
    "    )\n",
    "    print(\"Dataset loaded. Total samples:\", len(dataset))\n",
    "    losses_apdt = []\n",
    "    losses_trans = []\n",
    "    losses_tcn = []\n",
    "    losses_ens = []\n",
    "    \n",
    "    cv_index = 0\n",
    "    for train_idx, val_idx in time_series_splits(dataset, train_size, step_size, max_splits=3):\n",
    "        cv_index += 1\n",
    "        print(f\"\\n--- CV Split {cv_index} ---\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model_apdt = APDTFlow(\n",
    "            forecast_horizon=T_out,\n",
    "            **apdtflow_params\n",
    "        ).to(device)\n",
    "        try:\n",
    "            model_trans = TransformerForecaster(\n",
    "                input_dim=transformer_params[\"input_dim\"],\n",
    "                model_dim=transformer_params[\"model_dim\"],\n",
    "                num_layers=transformer_params[\"num_layers\"],\n",
    "                nhead=transformer_params[\"nhead\"],\n",
    "                forecast_horizon=T_out\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "            print(\"TransformerForecaster error:\", e)\n",
    "            model_trans = None\n",
    "        try:\n",
    "            model_tcn = TCNForecaster(\n",
    "                input_channels=tcn_params[\"input_channels\"],\n",
    "                num_channels=tcn_params[\"num_channels\"],\n",
    "                kernel_size=tcn_params[\"kernel_size\"],\n",
    "                forecast_horizon=T_out\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "            print(\"TCNForecaster error:\", e)\n",
    "            model_tcn = None\n",
    "        \n",
    "        ensemble_models = []\n",
    "        if model_apdt is not None:\n",
    "            ensemble_models.append(model_apdt)\n",
    "        if model_trans is not None:\n",
    "            ensemble_models.append(model_trans)\n",
    "        if model_tcn is not None:\n",
    "            ensemble_models.append(model_tcn)\n",
    "        try:\n",
    "            model_ens = EnsembleForecaster(models=ensemble_models).to(device)\n",
    "        except Exception as e:\n",
    "            print(\"EnsembleForecaster error:\", e)\n",
    "            model_ens = None\n",
    "        \n",
    "        print(\"\\nTraining APDTFlow model...\")\n",
    "        loss_apdt = train_on_split(model_apdt, train_loader, val_loader, num_epochs, learning_rate, device)\n",
    "        losses_apdt.append(loss_apdt)\n",
    "        \n",
    "        if model_trans is not None:\n",
    "            print(\"\\nTraining TransformerForecaster model...\")\n",
    "            loss_trans = train_on_split(model_trans, train_loader, val_loader, num_epochs, learning_rate, device)\n",
    "            losses_trans.append(loss_trans)\n",
    "        else:\n",
    "            losses_trans.append(np.nan)\n",
    "            \n",
    "        if model_tcn is not None:\n",
    "            print(\"\\nTraining TCNForecaster model...\")\n",
    "            loss_tcn = train_on_split(model_tcn, train_loader, val_loader, num_epochs, learning_rate, device)\n",
    "            losses_tcn.append(loss_tcn)\n",
    "        else:\n",
    "            losses_tcn.append(np.nan)\n",
    "            \n",
    "        if model_ens is not None:\n",
    "            loss_ens = evaluate_model(model_ens, val_loader, device)\n",
    "            losses_ens.append(loss_ens)\n",
    "            print(f\"\\nEnsembleForecaster CV Loss: {loss_ens:.4f}\")\n",
    "        else:\n",
    "            losses_ens.append(np.nan)\n",
    "        \n",
    "        if cv_index <= max_cv_plots:\n",
    "            full_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "            x_batch, y_batch = next(iter(full_loader))\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            if x_batch.dim() == 4 and x_batch.size(1) == 1:\n",
    "                x_batch = x_batch.squeeze(1)\n",
    "            _, T_in_current = x_batch.shape[1:]\n",
    "            t_span = torch.linspace(0, 1, steps=T_in, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                preds_apdt, _ = model_apdt(x_batch, t_span)\n",
    "            save_forecast_plot(x_batch, y_batch, preds_apdt, T_in, T_out,\n",
    "                               os.path.join(plots_dir, f\"APDTFlow_Forecast_Horizon_{T_out}_CV{cv_index}.png\"),\n",
    "                               title=f\"APDTFlow Forecast (Horizon {T_out}, CV{cv_index})\")\n",
    "            \n",
    "            if model_trans is not None:\n",
    "                with torch.no_grad():\n",
    "                    preds_trans = run_model_forward(model_trans, x_batch, t_span, device)\n",
    "                save_forecast_plot(x_batch, y_batch, preds_trans, T_in, T_out,\n",
    "                                   os.path.join(plots_dir, f\"Transformer_Forecast_Horizon_{T_out}_CV{cv_index}.png\"),\n",
    "                                   title=f\"Transformer Forecast (Horizon {T_out}, CV{cv_index})\")\n",
    "            \n",
    "            if model_tcn is not None:\n",
    "                with torch.no_grad():\n",
    "                    preds_tcn = run_model_forward(model_tcn, x_batch, t_span, device)\n",
    "                save_forecast_plot(x_batch, y_batch, preds_tcn, T_in, T_out,\n",
    "                                   os.path.join(plots_dir, f\"TCN_Forecast_Horizon_{T_out}_CV{cv_index}.png\"),\n",
    "                                   title=f\"TCN Forecast (Horizon {T_out}, CV{cv_index})\")\n",
    "            \n",
    "            if model_ens is not None:\n",
    "                with torch.no_grad():\n",
    "                    preds_ens, _ = model_ens.predict(x_batch, T_out, device)\n",
    "                save_forecast_plot(x_batch, y_batch, preds_ens, T_in, T_out,\n",
    "                                   os.path.join(plots_dir, f\"Ensemble_Forecast_Horizon_{T_out}_CV{cv_index}.png\"),\n",
    "                                   title=f\"Ensemble Forecast (Horizon {T_out}, CV{cv_index})\")\n",
    "    \n",
    "    avg_loss_apdt = np.nanmean(losses_apdt)\n",
    "    avg_loss_trans = np.nanmean(losses_trans)\n",
    "    avg_loss_tcn = np.nanmean(losses_tcn)\n",
    "    avg_loss_ens = np.nanmean(losses_ens)\n",
    "    \n",
    "    results.append({\n",
    "        \"Forecast_Horizon\": T_out,\n",
    "        \"Model\": \"APDTFlow\",\n",
    "        \"Avg_Validation_Loss\": avg_loss_apdt\n",
    "    })\n",
    "    results.append({\n",
    "        \"Forecast_Horizon\": T_out,\n",
    "        \"Model\": \"TransformerForecaster\",\n",
    "        \"Avg_Validation_Loss\": avg_loss_trans\n",
    "    })\n",
    "    results.append({\n",
    "        \"Forecast_Horizon\": T_out,\n",
    "        \"Model\": \"TCNForecaster\",\n",
    "        \"Avg_Validation_Loss\": avg_loss_tcn\n",
    "    })\n",
    "    results.append({\n",
    "        \"Forecast_Horizon\": T_out,\n",
    "        \"Model\": \"EnsembleForecaster\",\n",
    "        \"Avg_Validation_Loss\": avg_loss_ens\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Results:\n",
      "    Forecast_Horizon                  Model  Avg_Validation_Loss\n",
      "0                  7               APDTFlow             1.985979\n",
      "1                  7  TransformerForecaster            12.408953\n",
      "2                  7          TCNForecaster            13.574383\n",
      "3                  7     EnsembleForecaster            12.521067\n",
      "4                 14               APDTFlow             2.158489\n",
      "5                 14  TransformerForecaster            12.617352\n",
      "6                 14          TCNForecaster            13.084925\n",
      "7                 14     EnsembleForecaster            11.993816\n",
      "8                 30               APDTFlow             2.002482\n",
      "9                 30  TransformerForecaster            14.257534\n",
      "10                30          TCNForecaster            16.954437\n",
      "11                30     EnsembleForecaster            13.411902\n",
      "Results table saved to: C:\\Users\\yotam\\experiments\\results_experiment.csv\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"Experiment Results:\")\n",
    "print(results_df)\n",
    "\n",
    "results_csv_path = os.path.join(results_dir, \"results_experiment.csv\")\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(\"Results table saved to:\", results_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss comparison plot saved to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Validation_Loss_Comparison.png\n",
      "Performance vs. Horizon plot saved to: C:/Users/yotam/code_projects/APDTFlow/experiments/results_plots\\Performance_vs_Horizon.png\n"
     ]
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=\"Forecast_Horizon\", y=\"Avg_Validation_Loss\", hue=\"Model\", data=results_df)\n",
    "plt.title(\"Average Validation Loss by Forecast Horizon and Model\")\n",
    "plt.xlabel(\"Forecast Horizon\")\n",
    "plt.ylabel(\"Avg Validation Loss\")\n",
    "plt.legend(title=\"Model\")\n",
    "bar_plot_path = os.path.join(plots_dir, \"Validation_Loss_Comparison.png\")\n",
    "plt.savefig(bar_plot_path)\n",
    "plt.close()\n",
    "print(\"Validation loss comparison plot saved to:\", bar_plot_path)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for model_name in results_df[\"Model\"].unique():\n",
    "    model_data = results_df[results_df[\"Model\"] == model_name]\n",
    "    plt.plot(model_data[\"Forecast_Horizon\"], model_data[\"Avg_Validation_Loss\"], marker=\"o\", label=model_name)\n",
    "plt.xlabel(\"Forecast Horizon\")\n",
    "plt.ylabel(\"Avg Validation Loss\")\n",
    "plt.title(\"Model Performance vs. Forecast Horizon\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "line_plot_path = os.path.join(plots_dir, \"Performance_vs_Horizon.png\")\n",
    "plt.savefig(line_plot_path)\n",
    "plt.close()\n",
    "print(\"Performance vs. Horizon plot saved to:\", line_plot_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
